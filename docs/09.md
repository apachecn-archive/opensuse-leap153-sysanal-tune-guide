# 9 内核控制组

> 原文：<https://doc.opensuse.org/documentation/leap/tuning/html/book-tuning/cha-tuning-cgroups.html>

###### 摘要

内核控制组( " cgroups " )是一个内核特性，用于分配和限制进程的硬件和系统资源。流程也可以用分层树结构来组织。

[9.1 O概述](cha-tuning-cgroups.html#sec-tuning-cgroups-overview)

[9.2 资源核算](cha-tuning-cgroups.html#sec-tuning-cgroups-accounting)

[9.3 设置资源限制](cha-tuning-cgroups.html#sec-tuning-cgroups-usage)

[9.4 使用 `TasksMax` 避免 fork bombs ](cha-tuning-cgroups.html#sec-tuning-cgroups-tasksmax)

[9.5 用比例权重策略控制 I/O](cha-tuning-cgroups.html#id-1.7.6.3.7)

[9.6 更多信息](cha-tuning-cgroups.html#id-1.7.6.3.8)

## 9.1 概述

每个进程被分配一个管理组。cgroups 在分层树结构中排序。您可以为单个进程或层次树的整个分支设置资源限制，如 CPU、内存、磁盘 I/O 或网络带宽使用。

在 openSUSE Leap ，`systemd`使用 cgroups 将所有进程组织成组，`systemd`称之为切片。`systemd`还提供了设置 cgroup 属性的接口。

命令`systemd-cgls`显示层级树。

内核提供了两个版本的 cgroup APIs。它们提供的 cgroup 属性不同，控制器层次的组织也不同。试图消除差异。默认情况下，`systemd`在 _混合_ 模式下运行，这意味着控制器通过 v1 API 使用。cgroup v2 仅用于 systemd 自己的跟踪。通过 v2 API 使用控制器时，还有 _统一_ 模式。您只能设置一种模式。

要启用统一控制组层次结构，将`systemd.unified_cgroup_hierarchy=1`作为内核命令行参数添加到 GRUB 2 引导加载程序。(参考书*参考*，第 12 章“引导加载程序 GRUB 2”了解更多关于配置 GRUB 2 的细节。)

## 9.2 资源核算

将进程组织成不同的 cgroup 可用于获取每个 cgroup 的资源消耗数据。

会计具有相对较小但非零的开销，其影响取决于工作负载。激活一个单元的核算也将隐式激活同一切片中的所有单元、其所有父切片以及其中包含的单元。

可以使用指令(如`MemoryAccounting=`)按单位设置会计，或者使用指令`DefaultMemoryAccounting=`对`/etc/systemd/system.conf`中的所有单位进行全局设置。参考`man systemd.resource-control`获取可能指令的详细列表。

## 9.3 设置资源限制

![Note](img/ff87f0b59d655d477bfebbc447e7a566.png "Note")

###### 注意:隐含的资源消耗

请注意，资源消耗隐含地取决于工作负载执行的环境(例如，库/内核中数据结构的大小、实用程序的分叉行为、计算效率)。因此，如果环境发生变化，建议(重新)校准您的限值。

可以使用`systemctl set-property`命令设置对 cgroups 的限制。语法是:

```sh
# systemctl set-property [--runtime] *NAME* *PROPERTY1*=*VALUE* [*PROPERTY2*=*VALUE*]
```

配置的值会立即应用。可选地，使用`--runtime`选项，以便新值在重启后不会持续。

用一个`systemd`服务、作用域或片名替换*名*。

有关属性的完整列表和更多详细信息，请参见`man systemd.resource-control`。

## 9.4 使用`TasksMax`避免 fork bombs

`systemd`支持为每个单独的叶单元配置任务计数限制，或在片上聚合任务计数限制。上游`systemd`默认限制每个单元中的任务数量(内核全局限制的 15%，运行`/usr/sbin/sysctl kernel.pid_max`查看总限制)。每个用户的切片限制为内核限制的 33%。但是，这对于 SLE 来说就不一样了。

### 9.4.1 查找当前默认`TasksMax`值

在实践中，很明显没有一个单一的缺省适用于所有用例。 openSUSE Leap 附带了两个定制配置，它们覆盖了系统单元和用户片的上游默认值，并将它们都设置为`infinity`。`/usr/lib/systemd/system.conf.d/__25-defaults-SLE.conf` 包含这些行:

```sh
[Manager]
DefaultTasksMax=infinity
```

`/usr/lib/systemd/system/user-.slice.d/25-defaults-SLE.conf` 包含这几行:

```sh
[Slice]
TasksMax=infinity
```

使用`systemctl`验证 DefaultTasksMax 值:

```sh
> systemctl show --property DefaultTasksMax
DefaultTasksMax=infinity
```

`infinity`表示没有限制。不要求更改默认值，但设置一些限制可能有助于防止系统因进程失控而崩溃。

### 9.4.2 覆盖`DefaultTasksMax`值

通过创建一个新的覆盖文件`/etc/systemd/system.conf.d/90-system-tasksmax.conf`来更改全局`DefaultTasksMax`值，并编写以下代码行来设置一个新的默认限制，即每个系统单元 256 个任务:

```sh
[Manager]
DefaultTasksMax=256
```

加载新设置，然后验证它是否已更改:

```sh
> sudo systemctl daemon-reload
> systemctl show --property DefaultTasksMax
DefaultTasksMax=256
```

调整此默认值以满足您的需求。您可以根据需要对单个服务设置不同的限制。这个例子是针对 MariaDB 的。首先检查电流有效值:

```sh
> systemctl status mariadb.service
  ● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset>
   Active: active (running) since Tue 2020-05-26 14:15:03 PDT; 27min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 11845 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 256)
   CGroup: /system.slice/mariadb.service
           └─11845 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
```

Tasks 行显示 MariaDB 当前有 30 个任务在运行，并且有一个默认的上限 256，这对于一个数据库来说是不够的。以下示例演示了如何将 MariaDB 的限制提高到 8192。

```sh
> sudo systemctl set-property mariadb.service TasksMax=8192
> systemctl status mariadb.service
● mariadb.service - MariaDB database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disab>
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─50-TasksMax.conf
   Active: active (running) since Tue 2020-06-02 17:57:48 PDT; 7min ago
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/
  Process: 3446 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper upgrade (code=exited, sta>
  Process: 3440 ExecStartPre=/usr/lib/mysql/mysql-systemd-helper install (code=exited, sta>
 Main PID: 3452 (mysqld)
   Status: "Taking your SQL requests now..."
    Tasks: 30 (limit: 8192)
   CGroup: /system.slice/mariadb.service
           └─3452 /usr/sbin/mysqld --defaults-file=/etc/my.cnf --user=mysql
```

`systemctl set-property`应用新的限制，并为持久性创建一个插入文件`/etc/systemd/system/mariadb.service.d/50-TasksMax.conf`，其中只包含您想要应用到现有单元文件的更改。该值不必是 8192，而是应该是适合您的工作负载的任何限制。

### 9.4.3 默认`TasksMax`对用户的限制

用户的默认限制应该相当高，因为用户会话需要更多的资源。通过创建新文件为任何用户设置自己的默认值，例如`/etc/systemd/system/user-.slice.d/40-user-taskmask.conf`。以下示例将默认值设置为 16284:

```sh
[Slice]
TasksMax=16284
```

![Note](img/ff87f0b59d655d477bfebbc447e7a566.png "Note")

###### 注:数字前缀参考

请参见[https://documentation.SuSE.com/SLES/15-SP3/html/SLES-all/cha-systemd.html # sec-boot-systemd-custom-drop-in](https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-systemd.html#sec-boot-systemd-custom-drop-in)以了解插入文件的数字前缀。

然后重新加载 systemd 以加载新值，并验证更改:

```sh
> sudo systemctl daemon-reload
> systemctl show --property TasksMax user-1000.slice
TasksMax=16284
```

你怎么知道用什么值呢？这取决于您的工作负载、系统资源和其他资源配置。当您的`TasksMax`值过低时，您会看到错误消息，如 _无法分叉(资源暂时不可用)_ ， _无法创建线程来处理新连接_ ， _错误:函数调用‘fork’失败，错误代码为 11，‘资源暂时不可用’_。

有关在 systemd 中配置系统资源的更多信息，请参见`systemd.resource-control (5)`。

## 9.5 用比例权重策略控制 I/O

本节介绍如何使用 Linux 内核的块 I/O 控制器来区分 I/O 操作的优先级。cgroup blkio 子系统控制和监控块设备上的 I/O 访问。包含 cgroup 的子系统参数的状态对象被表示为 cgroup 虚拟文件系统(也称为伪文件系统)中的伪文件。

本节中的示例展示了如何将值写入这些伪文件来限制访问或带宽，以及如何从这些伪文件中读取值来提供有关 I/O 操作的信息。为 cgroup-v1 和 cgroup-v2 都提供了示例。

您需要一个测试目录，其中包含两个用于测试性能和更改设置的文件。创建完全由文本填充的测试文件的一种快速方法是使用`yes`命令。以下示例命令创建一个测试目录，然后用两个 537 MB 的文本文件填充它:

```sh
host1:~ # mkdir /io-cgroup
host1:~ # cd /io-cgroup
host1:~ # yes this is a test file | head -c 537MB > file1.txt
host1:~ # yes this is a test file | head -c 537MB > file2.txt
```

要运行这些示例，请打开三个命令 shells。两个 shell 用于读取器进程，一个 shell 用于运行控制 I/O 的步骤。在示例中，每个命令提示符都进行了标记，以指示它是否代表一个读取器进程或 I/O。

### 9.5.1 使用 cgroup-v1

以下比例权重策略文件可用于授予某个读取器进程比访问同一磁盘的其他读取器进程更高的 I/O 操作优先级。

- `blkio.bfq.weight`(在使用 blk-mq 的 5.0 版内核中以及使用 BFQ I/O 调度程序时可用)

为了测试这一点，在不控制 I/O 的情况下运行单个读取器进程(在示例中，从 SSD 读取)，使用`file2.txt`:

```sh
[io-controller] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[io-controller] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.33049 s, 404 MB/s
```

现在运行后台进程从同一个磁盘读取数据:

```sh
[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.61592 s, 205 MB/s
```

每个进程获得 I/O 操作吞吐量的一半。接下来，设置两个控制组—每个进程一个—验证是否使用了 BFQ，并为 reader2 设置不同的权重:

```sh
[io-controller] host1:/io-cgroup # cd /sys/fs/cgroup/blkio/
[io-controller] host1:/sys/fs/cgroup/blkio/ # mkdir reader1
[io-controller] host1:/sys/fs/cgroup/blkio/ # mkdir reader2
[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 5220 > reader1/cgroup.procs
[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 5251 > reader2/cgroup.procs
[io-controller] host1:/sys/fs/cgroup/blkio/ # cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader1/blkio.bfq.weight
100
[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 200 > reader2/blkio.bfq.weight
[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader2/blkio.bfq.weight
200
```

有了这些设置和后台的 reader1，reader2 应该具有比以前更高的吞吐量:

```sh
[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 2.06604 s, 260 MB/s
```

较高的比例权重导致读取器 2 的吞吐量较高。现在它的重量再增加一倍:

```sh
[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader1/blkio.bfq.weight
100
[io-controller] host1:/sys/fs/cgroup/blkio/ # echo 400 > reader2/blkio.bfq.weight
[io-controller] host1:/sys/fs/cgroup/blkio/ # cat reader2/blkio.bfq.weight
400
```

这导致了读取器 2 的吞吐量的另一个增加:

```sh
[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
5220
...
[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5251
131072+0 records in
131072+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 1.69026 s, 318 MB/s
```

### 9.5.2 使用 cgroup-v2

首先按照本章开头所示设置您的测试环境。

然后，确保块 IO 控制器未处于活动状态，因为这是针对 cgroup-v1 的。为此，用内核参数`cgroup_no_v1=blkio`引导。验证是否使用了此参数，以及 IO 控制器(cgroup-v2)是否可用:

```sh
[io-controller] host1:/io-cgroup # cat /proc/cmdline
BOOT_IMAGE=... cgroup_no_v1=blkio ...
[io-controller] host1:/io-cgroup # cat /sys/fs/cgroup/unified/cgroup.controllers
io
```

接下来，启用 IO 控制器:

```sh
[io-controller] host1:/io-cgroup # cd /sys/fs/cgroup/unified/
[io-controller] host1:/sys/fs/cgroup/unified # echo '+io' > cgroup.subtree_control
[io-controller] host1:/sys/fs/cgroup/unified # cat cgroup.subtree_control
io
```

现在运行所有的测试步骤，类似于 cgroup-v1 的步骤。请注意，有些目录是不同的。使用 file2.txt 运行单个读取器进程(在示例中，从 SSD 读取数据),而不控制其 I/O:

```sh
[io-controller] host1:/sys/fs/cgroup/unified # cd -
[io-controller] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[io-controller] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5633
[...]
```

运行从同一磁盘读取数据的后台进程，并记下您的吞吐量值:

```sh
[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
5633
[...]
[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
5703
[...]
```

每个进程获得 I/O 操作吞吐量的一半。设置两个控制组—每个进程一个—验证 BFQ 是活动调度程序，并为 reader2 设置不同的权重:

```sh
[io-controller] host1:/io-cgroup # cd -
[io-controller] host1:/sys/fs/cgroup/unified # mkdir reader1
[io-controller] host1:/sys/fs/cgroup/unified # mkdir reader2
[io-controller] host1:/sys/fs/cgroup/unified # echo 5633 > reader1/cgroup.procs
[io-controller] host1:/sys/fs/cgroup/unified # echo 5703 > reader2/cgroup.procs
[io-controller] host1:/sys/fs/cgroup/unified # cat /sys/block/sda/queue/scheduler
mq-deadline kyber [bfq] none
[io-controller] host1:/sys/fs/cgroup/unified # cat reader1/io.bfq.weight
default 100
[io-controller] host1:/sys/fs/cgroup/unified # echo 200 > reader2/io.bfq.weight
[io-controller] host1:/sys/fs/cgroup/unified # cat reader2/io.bfq.weight
default 200
```

使用新设置测试您的吞吐量。reader2 应该显示吞吐量增加。

```sh
[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[reader1] host1:/io-cgroup # echo $$; dd if=file1 of=/dev/null bs=4k
5633
[...]
[reader2] host1:/io-cgroup # echo $$; dd if=file2 of=/dev/null bs=4k count=131072
5703
[...]
```

尝试再次加倍 reader2 的权重，并测试新设置:

```sh
[reader2] host1:/io-cgroup # echo 400 > reader1/blkio.bfq.weight
[reader2] host1:/io-cgroup # cat reader2/blkio.bfq.weight
400
[reader1] host1:/io-cgroup # sync; echo 3 > /proc/sys/vm/drop_caches
[reader1] host1:/io-cgroup # echo $$; dd if=file1.txt of=/dev/null bs=4k
[...]
[reader2] host1:/io-cgroup # echo $$; dd if=file2.txt of=/dev/null bs=4k count=131072
[...]
```

## 9.6 更多信息

- 内核文档(包`kernel-source`):文件`/usr/src/linux/Documentation/admin-guide/cgroup-v1`和文件`/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst`。

- [https://lwn.net/Articles/604609/](https://lwn.net/Articles/604609/)——布朗、尼尔:控制组系列(2014，7 集)。

- [https://lwn.net/Articles/243795/]()——控制容器中的内存使用(2007)。

- [https://lwn.net/Articles/236038/]()——过程容器(2007)。
